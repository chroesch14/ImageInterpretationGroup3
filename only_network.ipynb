{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a4a07cf",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71441df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import moduels\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import math\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52b80b",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48d88ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thes shape of the images is:\n",
      "(3, 10980, 10980, 3)\n"
     ]
    }
   ],
   "source": [
    "# access the data\n",
    "# this part have to be replaced when the images from the neuronal networks are available\n",
    "dset = h5py.File(\"dataset_test.h5\",\"r\")\n",
    "\n",
    "#Access to the input data\n",
    "RGB = dset[\"RGB\"]\n",
    "NIR = dset[\"NIR\"]\n",
    "print(\"Thes shape of the images is:\")\n",
    "print(RGB.shape)\n",
    "\n",
    "\n",
    "# get size and numbers of the dataset\n",
    "# number of images\n",
    "n_images = RGB.shape[0]\n",
    "\n",
    "# imagesize x_direction\n",
    "sx_image = RGB.shape[1]\n",
    "#imageszize y_direction\n",
    "sy_image = RGB.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88fba4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of images in x and y direction\n",
    "\n",
    "# number of images in x direction\n",
    "nx = 5\n",
    "#number of images in y direction \n",
    "ny = 5\n",
    "\n",
    "# size of patch\n",
    "# alle sind quadratisch\n",
    "s_patch = 256\n",
    "\n",
    "# numbers of categories in the pretrained network Pascal Voc --> 21\n",
    "n_cat = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdf8cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the coordinates of all top left pixels of each patch\n",
    "# is for each image the same\n",
    "\n",
    "x_max = sx_image-s_patch\n",
    "y_max = sy_image-s_patch \n",
    "\n",
    "stepsize_x = math.floor(x_max/nx)\n",
    "stepsize_y = math.floor(y_max/ny)\n",
    "\n",
    "#create arrays with the corrdinates\n",
    "x_coordinates = np.arange(0,x_max,stepsize_x)\n",
    "y_coordinates = np.arange(0,y_max,stepsize_y)\n",
    "\n",
    "# create grid\n",
    "xx , yy = np.meshgrid(x_coordinates,y_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f74cf8",
   "metadata": {},
   "source": [
    "# Network part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd470e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networks modules\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29988931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model\n",
    "# available more models are avialable at: https://pytorch.org/vision/stable/models.html\n",
    "\n",
    "#FCN ResNet50\n",
    "model = models.segmentation.fcn_resnet50(pretrained=True, progress=True)\n",
    "\n",
    "#DeepLabV3 ResNet50\n",
    "#model = models.segmentation.deeplabv3_resnet50(pretrained=True, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b8444a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to prepare the input image into the correct form (for all segementation models of torchvision the same normalistation and standard deviation can be used!)\n",
    "# more information: https://learnopencv.com/pytorch-for-beginners-image-classification-using-pre-trained-models/\n",
    "# or https://pytorch.org/vision/stable/models.html\n",
    "trf = T.Compose([T.Resize(s_patch),\n",
    "                 T.CenterCrop(s_patch),\n",
    "                 T.ToTensor(), \n",
    "                 T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                             std = [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8da392e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run over each image patch and store it\n",
    "\n",
    "for i in range(len(yy)):\n",
    "    for j in range(len(xx)):\n",
    "        # Achtung muss hier noch angepasst werden um Ã¼ber alle Bilder zu iterien --> einfachster Weg for loop\n",
    "        temp_img = Image.fromarray(RGB[1,xx[i][j]:xx[i][j]+s_patch, yy[i][j]:yy[i][j]+s_patch,:3])\n",
    "        inp = trf(temp_img).unsqueeze(0) #check what unsqueeze does\n",
    "        out = model(inp)['out']\n",
    "        \n",
    "        # reshape the output of the network to use it in the classifier \n",
    "        # size (image_size x image_size) x 21 (all of the torchvision segmentation models are trained with the Pascal Voc dataseet --> contains 21 categories) more info: https://pytorch.org/vision/stable/models.html\n",
    "        \n",
    "        # TODO check if the reshape is done in the correct way\n",
    "        temp = out.detach().numpy()\n",
    "        temp1 = temp[0]\n",
    "        temp2 = temp1.reshape(np.square(s_patch),n_cat)\n",
    "        \n",
    "        # save array\n",
    "        np.save('./output_network/'+str(xx[i][j])+'_'+str(yy[i][j])+'.npy',temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a656f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.load('test_ndarray.npy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
